# Rationalit√©-des-LLM
Ce projet analyse la rationalit√© des IA (Gemini, ChatGPT, Mistral) en comparant leurs choix √† 2929 d√©cisions humaines dans un sc√©nario de billets (prix, temps, correspondances, confort). Avec RprobitB, il explore leur potentiel en √©tudes de march√© pour mod√©liser les pr√©f√©rences √©conomiques.

# M√©thodologie
### Dataset
Le dataset original, t√©l√©charg√© depuis le repository GitHub de Lo√´l Schlaeger (https://github.com/loelschlaeger/RprobitB/blob/main/data/train_choice.rda), contient des donn√©es de choix humains dans un sc√©nario de s√©lection entre deux options de billets de train (A et B).
R√©sum√© des Variables

    Prix (price_A, price_B) : Co√ªt du billet pour chaque option (en euros).
    Dur√©e du trajet (time_A, time_B) : Dur√©e totale du voyage (en heures).
    Changements n√©cessaires (change_A, change_B) : Nombre de correspondances pour effectuer le trajet.
    Niveau de confort (comfort_A, comfort_B) : (mesure invers√©e) √âvalu√© sur une √©chelle de 2 √† 0, o√π 0 repr√©sente le confort maximal et 2 le confort minimal.

Le dataset, initialement au format .rda, a √©t√© transform√© en fichier CSV pour faciliter son utilisation dans des scripts Python. Cette transformation permet d‚Äôint√©grer les donn√©es dans des workflows d‚Äôanalyse et de simulation avec des mod√®les d‚Äôintelligence artificielle.


---

### **Fonctionnement des API et Explication des Param√®tres**

Dans cette √©tude, trois mod√®les d‚Äôintelligence artificielle (**ChatGPT**, **Gemini**, et **Mistral**) ont √©t√© utilis√©s pour simuler des choix humains entre deux options de billets (**A** et **B**). Chaque IA a re√ßu les variables des billets (prix, temps de trajet, changements, confort) sous forme de prompts clairs, et leurs r√©ponses ont √©t√© enregistr√©es pour une analyse comparative avec les choix humains.

---

### **Prompt Commun**
Pour les trois API, le m√™me **prompt** a √©t√© utilis√© pour garantir une coh√©rence dans la fa√ßon dont les donn√©es sont pr√©sent√©es aux mod√®les. Chaque prompt inclut les crit√®res suivants pour contextualiser la t√¢che :

```python
"Tu es un passager de train et tu dois choisir entre deux options pour ton voyage. Voici les crit√®res importants :
1. Prix : Le co√ªt du billet en euros. Un prix plus bas est pr√©f√©r√©.
2. Temps : La dur√©e totale du trajet en heures. Un trajet plus court est pr√©f√©r√©.
3. Changements : Le nombre de correspondances pendant le trajet. Moins de changements sont pr√©f√©r√©s.
4. Confort : Une mesure invers√©e du confort (plus bas = plus confortable)."
```

Le contenu du prompt, propre √† chaque ligne de donn√©es, est ensuite int√©gr√© comme suit :
```python
prompt = f"""
Voici deux options‚ÄØ:
- Option A : Prix = {row['price_A']} ‚Ç¨, Temps = {row['time_A']} heures, Changements = {row['change_A']}, Confort = {row['comfort_A']}
- Option B : Prix = {row['price_B']} ‚Ç¨, Temps = {row['time_B']} heures, Changements = {row['change_B']}, Confort = {row['comfort_B']}
Bas√© uniquement sur ces informations selon tes crit√®res, quelle option choisirais-tu‚ÄØ? R√©pondez par 'A' ou 'B' seulement.
"""
```

---

### **Choix de la Temp√©rature (0.7)**

La temp√©rature est un param√®tre essentiel qui contr√¥le la **variabilit√© des r√©ponses** d‚Äôune IA. Apr√®s plusieurs tests avec diff√©rentes valeurs, le choix de **0.7** a √©t√© retenu pour les raisons suivantes :

- **Temp√©rature 0 :** Produisait des r√©ponses trop rigides et d√©terministes, limitant l‚Äôadaptabilit√© des mod√®les.
- **Temp√©rature 1 :** Menait √† des hallucinations fr√©quentes (comme produire des phrases compl√®tes ou ignorer les consignes de r√©pondre uniquement par **A** ou **B**).
- **Temp√©rature 0.7 :** Offrait un compromis id√©al entre diversit√© et coh√©rence :
  - R√©duisait les hallucinations.
  - Garantissait que les mod√®les respectaient mieux les consignes.
  - Bien que certains mod√®les, comme **Mistral**, puissent mieux performer √† **1**, le choix de **0.7** a permis d‚Äôuniformiser les r√©sultats entre tous les mod√®les pour une meilleure coh√©rence globale.

---
### **Pourquoi traiter par batch ?**

Le traitement par batch permet de :
1. **Respecter les limites des API** (ex. 1 requ√™te/sec pour Mistral, 15 req/min pour Gemini) en ajoutant des pauses.
2. **G√©rer les erreurs** en identifiant et isolant les lignes probl√©matiques sans interrompre tout le processus.
3. **Sauvegarder les r√©sultats progressivement** pour √©viter la perte de donn√©es en cas d'interruption.
4. **Optimiser les performances** en assurant un traitement fluide sans surcharger les API.

### **Explication de Max Tokens**

Le param√®tre **Max Tokens** d√©finit la longueur maximale de la r√©ponse g√©n√©r√©e par l'IA. Dans ce projet, il est fix√© √† **10 tokens** , meme si theoriquement il pourrais ete fix√© a **1 token** car le prompte limite les r√©ponses √† une seule lettre (**A** ou **B**).
## **API Mistral**

#### **Caract√©ristiques**
- **Mod√®le utilis√© :** `mistral-small-latest`  
- **Param√®tres :**
  - **Temp√©rature :** 0.7 
- **Particularit√©s :**
  - **API gratuite**
  - Limite : **1 requ√™te par seconde**

#### **Code Complet**
```python
import requests
import pandas as pd
import time

api_url = "https://api.mistral.ai/v1/chat/completions"
api_key = "Votre_cl√©_API"

headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json",
}

file_path = r"C: le chemin de train_choice.csv"
dataset = pd.read_csv(file_path)

def get_ai_decision(row):
    prompt = f"""
    Tu es un passager de train et tu dois choisir entre deux options pour ton voyage. Voici les crit√®res importants :
    1. Prix : Le co√ªt du billet en euros. Un prix plus bas est pr√©f√©r√©.
    2. Temps : La dur√©e totale du trajet en heures. Un trajet plus court est pr√©f√©r√©.
    3. Changements : Le nombre de correspondances pendant le trajet. Moins de changements sont pr√©f√©r√©s.
    4. Confort : Une mesure invers√©e du confort (plus bas = plus confortable).

    Voici deux options‚ÄØ:
    - Option A : Prix = {row['price_A']} ‚Ç¨, Temps = {row['time_A']} heures, Changements = {row['change_A']}, Confort = {row['comfort_A']}
    - Option B : Prix = {row['price_B']} ‚Ç¨, Temps = {row['time_B']} heures, Changements = {row['change_B']}, Confort = {row['comfort_B']}

    Bas√© uniquement sur ces informations selon tes crit√®res, quelle option choisirais-tu‚ÄØ? R√©pondez par 'A' ou 'B' seulement.
    """
    payload = {
        "model": "mistral-small-latest",
        "messages": [
            {"role": "system", "content": "Tu es un assistant qui aide √† prendre des d√©cisions bas√©es sur des informations simples."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 10,
        "temperature": 0.7
    }
    try:
        response = requests.post(api_url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"Erreur avec l'API pour la ligne {row.name} : {e}")
        return "Erreur"

output_path = r"C:\Users\mistral_choice.csv" # a modifier selon le fichier d'output voulu

with open(output_path, "w") as f:
    f.write(",".join(dataset.columns) + ",ai_choice\n")

for index, row in dataset.iterrows():
    ai_choice = get_ai_decision(row)
    dataset.loc[index, "ai_choice"] = ai_choice
    with open(output_path, "a") as f:
        f.write(",".join(map(str, row.tolist())) + f",{ai_choice}\n")
    print(f"Ligne {index + 1}/{len(dataset)} trait√©e : {ai_choice}")
    time.sleep(2)

print(f"Traitement termin√©. Fichier final sauvegard√© dans : {output_path}")
```

---

### **API ChatGPT**

#### **Caract√©ristiques**
- **Mod√®le utilis√© :** `gpt-4o`  
- **Param√®tres :**
  - **Temp√©rature :** 0.7
- **Particularit√©s :**
  - **API payante**
  - Co√ªt total estim√© : **1.80 ‚Ç¨** pour traiter 2929 lignes

#### **Code Complet**
```python
from openai import OpenAI
import pandas as pd
import time

client = OpenAI(api_key="Votre_cl√©_API")

file_path = r"C: le chemin de train_choice.csv"
dataset = pd.read_csv(file_path)

def get_ai_decision(row):
    prompt = f"""
    Tu es un passager de train et tu dois choisir entre deux options pour ton voyage. Voici les crit√®res importants :
    1. Prix : Le co√ªt du billet en euros. Un prix plus bas est pr√©f√©r√©.
    2. Temps : La dur√©e totale du trajet en heures. Un trajet plus court est pr√©f√©r√©.
    3. Changements : Le nombre de correspondances pendant le trajet. Moins de changements sont pr√©f√©r√©s.
    4. Confort : Une mesure invers√©e du confort (plus bas = plus confortable).

    Voici deux options‚ÄØ:
    - Option A : Prix = {row['price_A']} ‚Ç¨, Temps = {row['time_A']} heures, Changements = {row['change_A']}, Confort = {row['comfort_A']}
    - Option B : Prix = {row['price_B']} ‚Ç¨, Temps = {row['time_B']} heures, Changements = {row['change_B']}, Confort = {row['comfort_B']}

    En tant que passager, quelle option choisirais-tu‚ÄØ? R√©ponds par 'A' ou 'B' seulement.
    """
    try:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Tu es un assistant qui aide √† prendre des d√©cisions bas√©es sur des informations simples"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=10,
            temperature=0.7
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"Erreur avec l'API pour la ligne {row.name} : {e}")
        return "Erreur"

output_path = r"C:\Users\amirb\Downloads\train_chatgpt.csv"

for index, row in dataset.iterrows():
    ai_choice = get_ai_decision(row)
    dataset.loc[index, "ai_choice_gpt"] = ai_choice
    print(f"Choix enregistr√© pour la ligne {index + 1}/{len(dataset)} : {ai_choice}")
    time.sleep(1)

dataset.to_csv(output_path, index=False)
print(f"Traitement termin√©. Fichier final sauvegard√© dans : {output_path}")
```

---

### **API Gemini**

#### **Caract√©ristiques**
- **Mod√®le utilis√© :** `gemini-1.5-flash`  
- **Param√®tres :**
  - **Temp√©rature :** 0.7  
- **Particularit√©s :**
  - Limite : **15 requ√™tes par minute**  
  - Limite : **1 000 000 tokens par minute**  
  - Limite : **1 500 requ√™tes par jour**  

#### **Code Complet**
```python
import time
import google.generativeai as genai
import pandas as pd

# Configurer l'API Gemini
api_key = "Votre_cl√©_API"  # Remplace par ta cl√©
genai.configure(api_key=api_key)

# Charger les donn√©es d'origine
file_path = r"C: le chemin de train_choice.csv"
dataset = pd.read_csv(file_path)

# Fonction pour interroger l'API Gemini
def get_ai_decision(row):
    prompt = f"""
    Tu es un passager de train et tu dois choisir entre deux options pour ton voyage. Voici les crit√®res importants :
    1. Prix : Le co√ªt du billet en euros. Un prix plus bas est pr√©f√©r√©.
    2. Temps : La dur√©e totale du trajet en heures. Un trajet plus court est pr√©f√©r√©.
    3. Changements : Le nombre de correspondances pendant le trajet. Moins de changements sont pr√©f√©r√©s.
    4. Confort : Une mesure invers√©e du confort (plus bas = plus confortable).

    Voici deux options disponibles :
    - Option A : Prix = {row['price_A']} ‚Ç¨, Temps = {row['time_A']} heures, Changements = {row['change_A']}, Confort = {row['comfort_A']}
    - Option B : Prix = {row['price_B']} ‚Ç¨, Temps = {row['time_B']} heures, Changements = {row['change_B']}, Confort = {row['comfort_B']}

    En tant que passager, quelle option choisirais-tu‚ÄØ? R√©ponds par 'A' ou 'B' seulement.
    """
    try:
        # Utiliser le mod√®le Gemini pour g√©n√©rer une r√©ponse
        model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config={
                "temperature": 0.7,
                "max_output_tokens": 10,
            },
        )
        chat_session = model.start_chat(history=[])
        response = chat_session.send_message(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"Erreur avec l'API pour la ligne {row.name} : {e}")
        return "Erreur"

# Sauvegarde ligne par ligne dans le fichier train_gemini.csv
output_path = r"C:train_gemini.csv"
with open(output_path, "w") as f:
    # √âcrire les en-t√™tes
    f.write(",".join(dataset.columns) + ",ai_choice\n")

# Traiter ligne par ligne avec respect des limites
for index, row in dataset.iterrows():
    ai_choice = get_ai_decision(row)
    dataset.loc[index, 'ai_choice'] = ai_choice
    # Sauvegarder la ligne dans le fichier
    with open(output_path, "a") as f:
        f.write(",".join(map(str, row.tolist())) + f",{ai_choice}\n")
    print(f"Ligne {index + 1}/{len(dataset)} trait√©e : {ai_choice}")
    
    # Pause pour respecter les limites de 15 requ√™tes par minute
    time.sleep(4)  # Pause de 4 secondes

print(f"Traitement termin√©. Fichier final sauvegard√© dans : {output_path}")
```
### **R√©sultats**

#### **Fusion des datasets g√©n√©r√©s**
Avant d‚Äôanalyser les r√©sultats, il est recommand√© de fusionner les trois datasets g√©n√©r√©s √† partir des API (**ChatGPT**, **Gemini**, et **Mistral**) en un seul dataset. Cela permet de faciliter les comparaisons et de calculer directement les correspondances entre les choix humains et ceux des mod√®les IA.

Le fichier fusionn√©, nomm√© **`train_human_vs_ia_merged.csv`**, contient :
- Les choix humains (**choice**) pour chaque ligne.
- Les choix simul√©s par chaque IA (**ai_choice_gpt**, **ai_choice_mistral**, **ai_choice_gemini**).

---

#### **Code pour calculer les correspondances**
Le code suivant calcule le pourcentage de correspondance entre les choix humains et ceux de chaque mod√®le IA :

```python
import pandas as pd

# Charger le dataset fusionn√©
file_path = r"C:\Users\amirb\Downloads\train_human_vs_ia_merged.csv"
dataset = pd.read_csv(file_path)

# Calculer les correspondances
for model in ['ai_choice_gpt', 'ai_choice_mistral', 'ai_choice_gemini']:
    match_percentage = (dataset['choice'] == dataset[model]).mean() * 100
    print(f"Pourcentage de correspondance entre Humain et {model} : {match_percentage:.2f}%")
```

---

#### **R√©sultats obtenus**
Voici les r√©sultats calcul√©s √† partir des donn√©es :

- **Correspondance entre Humains et ai_choice_gemini :** **65.59 %**
- **Correspondance entre Humains et ai_choice_gpt :** **65.28 %**
- **Correspondance entre Humains et ai_choice_mistral :** **62.34 %**

---

### **Analyse des variables choisies**

#### **Objectif**
Ce code analyse la distribution des valeurs des variables s√©lectionn√©es (**price**, **time**, **change**, et **comfort**) par les humains et les mod√®les IA (**ChatGPT**, **Gemini**, et **Mistral**). L‚Äôobjectif est de comparer comment chaque groupe (humains et mod√®les) priorise ces variables lors de la prise de d√©cision.

---

#### **Code pour l'analyse des distributions**

Voici le code utilis√© pour analyser et visualiser les distributions des valeurs choisies :

```python
import pandas as pd
import matplotlib.pyplot as plt

# Charger le dataset fusionn√©
file_path = r"C:\Users\amirb\Downloads\train_human_vs_ia_merged.csv"
dataset = pd.read_csv(file_path)

# Liste des variables et mod√®les √† analyser
variables = ['price', 'time', 'change', 'comfort']
models = ['choice', 'ai_choice_gpt', 'ai_choice_mistral', 'ai_choice-gemini']

# Ajouter des colonnes pour les variables choisies
for var in variables:
    for model in models:
        column_name = f"chosen_{var}_{model}"
        dataset[column_name] = dataset.apply(
            lambda row: row[f"{var}_A"] if row[model] == 'A' else row[f"{var}_B"],
            axis=1
        )

# Visualisation des distributions
for var in variables:
    plt.figure(figsize=(10, 6))
    for model in models:
        column_name = f"chosen_{var}_{model}"
        dataset[column_name].hist(alpha=0.5, label=model, bins=20)
    plt.title(f"Distribution de {var} choisi par les humains et les mod√®les")
    plt.xlabel(var.capitalize())


    plt.ylabel("Fr√©quence")
    plt.legend()
    plt.show()
```

---
### **M√©thode Probit avec {RprobitB}**

*Ce texte est une traduction en fran√ßais de la page officielle disponible sur [le d√©p√¥t GitHub de {RprobitB}](https://github.com/loelschlaeger/RprobitB/blob/main/README.md).*

---

#### **Objectif de {RprobitB}**
Le package **{RprobitB}** est con√ßu pour expliquer les choix effectu√©s par des d√©cideurs parmi un ensemble discret d'alternatives, en utilisant une approche bay√©sienne. Par exemple, il peut √™tre utilis√© pour comprendre pourquoi des touristes pr√©f√®rent un billet de train plut√¥t qu‚Äôun autre. Ces informations sont pr√©cieuses pour les entreprises ferroviaires, notamment pour √©valuer la **disposition √† payer** des clients pour des trajets plus rapides ou plus confortables.

---

#### **Installation**
Le package peut √™tre install√© directement depuis **CRAN** avec la commande suivante¬†:

```r
install.packages("RprobitB")
```

---

#### **Documentation**
La documentation compl√®te et les vignettes sont disponibles sur le site officiel du package : [Documentation RprobitB](https://loelschlaeger.de/RprobitB/).

---

#### **Exemple d‚Äôutilisation**
Dans cette √©tude, **{RprobitB}** a √©t√© utilis√© pour analyser un dataset contenant **2929 choix** effectu√©s par **235 individus n√©erlandais** confront√©s √† deux options de voyage en train. Les alternatives sont d√©crites par les variables suivantes :
- **Prix** (en euros)
- **Temps de trajet** (en heures)
- **Confort** (plus faible = plus confortable)
- **Nombre de correspondances**

---

#### **Ajustement du mod√®le Probit**
Un mod√®le probit est ajust√© pour expliquer les choix en fonction des variables mentionn√©es. La sp√©cification du mod√®le est donn√©e par la formule suivante :

```r
form <- choice ~ price + time + change + comfort | 0
```

Dans cette formule :
- **`choice`** est la variable cible (le choix effectu√© par les individus).
- **`price`, `time`, `change`, et `comfort`** sont les variables explicatives.

Le coefficient du **prix** est fix√© √† **-1** pour permettre une interpr√©tation mon√©taire des autres coefficients.

##### **Code pour ajuster le mod√®le**
```r
data <- prepare_data(form, train_choice, id = "deciderID", idc = "occasionID")
model <- fit_model(data, scale = "price := -1")
```

---

#### **R√©sultats estim√©s**
Les coefficients estim√©s montrent la valorisation des individus pour chaque variable en termes mon√©taires :
- **Une heure de trajet suppl√©mentaire** : ~25 ‚Ç¨
- **Une correspondance suppl√©mentaire** : ~5 ‚Ç¨
- **Un niveau de confort am√©lior√©** : ~15 ‚Ç¨

Ces valeurs permettent d‚Äô√©valuer la disposition √† payer des utilisateurs pour des am√©liorations sp√©cifiques des services.

##### **Visualisation des coefficients**
Les coefficients peuvent √™tre visualis√©s pour une meilleure interpr√©tation :

```r
plot(coef(model))
```

---

#### **Simulations de parts de march√©**
Gr√¢ce au mod√®le probit ajust√©, il est possible de simuler l‚Äôeffet de changements sp√©cifiques sur les parts de march√©.

1. **Augmentation du prix**
   Une augmentation du prix de **100 ‚Ç¨ √† 110 ‚Ç¨** (toutes choses √©gales par ailleurs) entra√Æne une perte de **15 %** des clients au profit de l‚Äôoption concurrente.

   ```r
   predict(
     model,
     data = data.frame(
       "price_A" = c(100, 110),
       "price_B" = c(100, 100)
     ),
     overview = FALSE
   )
   ```

2. **Am√©lioration du confort**
   En am√©liorant le confort de la classe pour compenser une augmentation de prix, on observe un **gain de 7 %** des parts de march√©.

   ```r
   predict(
     model,
     data = data.frame(
       "price_A" = c(100, 110),
       "comfort_A" = c(1, 0),
       "price_B" = c(100, 100),
       "comfort_B" = c(1, 1)
     ),
     overview = FALSE
   )
   ```

---

#### **Application dans notre √©tude**
Dans ce projet, la m√©thode probit a √©t√© utilis√©e pour analyser et comparer les choix humains et les d√©cisions g√©n√©r√©es par les mod√®les IA (**ChatGPT**, **Gemini**, et **Mistral**). L‚Äôobjectif est d‚Äô√©valuer la capacit√© des IA √† reproduire les priorit√©s humaines et de comprendre les diff√©rences dans la valorisation des variables.

Souhaitez-vous d'autres pr√©cisions ou des ajustements pour cette section ? üòä
